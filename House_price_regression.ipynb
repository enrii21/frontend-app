{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "5tHCY7H1kGkY",
        "XvFxkvxjku8f",
        "pFC0m9aCQqg0",
        "_MlrFGIJTWog",
        "Mp_CyiuBL5zX",
        "zVBIoG5gRJ89",
        "166AS9K1TeVj",
        "Qb2_881QVvwD",
        "WNfemhClWwgv",
        "hrc6EIMlbt7o",
        "Ur_shECudvw2",
        "1zafQpPheJDN",
        "nZU2wBKlfWNh",
        "FpjazabThzpv"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/enrii21/frontend-app/blob/main/House_price_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Environment Settings**"
      ],
      "metadata": {
        "id": "5tHCY7H1kGkY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This first part is aimed to set our working enviroment to easily download a dataset from Kaggle repositories, and import some useful Python libraries that we will meet later on."
      ],
      "metadata": {
        "id": "huIFTo0rlXIM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**title something**\n"
      ],
      "metadata": {
        "id": "R8KoT98QaToo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gf0BP8O7aaSr",
        "outputId": "d2f87380-079d-4ab0-8b46-74d0350be3df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Kaggle install and dataset import**"
      ],
      "metadata": {
        "id": "XvFxkvxjku8f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we just open a dialogue to upload our Kaggle API Token, which comes in the forms of a json document."
      ],
      "metadata": {
        "id": "QA5HLPypkStF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "TlHqdCaFD2t1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! ls -lha kaggle.json\n",
        "! pip install -q kaggle # installing the kaggle package\n",
        "! mkdir -p ~/.kaggle # creating .kaggle folder where the key should be placed\n",
        "! cp kaggle.json ~/.kaggle/ # move the key to the folder\n",
        "! pwd # checking the present working directory\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "collapsed": true,
        "id": "dZjLz7T1D6IK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! kaggle datasets download -d harlfoxem/housesalesprediction -p /content/input_data/house_sales"
      ],
      "metadata": {
        "collapsed": true,
        "id": "THYwyyCKMAcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! ls -l input_data/house_sales/housesalesprediction.zip\n",
        "! unzip input_data/house_sales/housesalesprediction.zip -d input_data/house_sales/"
      ],
      "metadata": {
        "collapsed": true,
        "id": "A3xieNoyEFqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install pandas numpy matplotlib seaborn scikit-learn"
      ],
      "metadata": {
        "collapsed": true,
        "id": "0R7PT6z2M9D0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this assignment, we will use a simplified data and create a simple **linear regression model**. The dataset can be downloaded from https://www.kaggle.com/harlfoxem/housesalesprediction/download.\n",
        "\n",
        "This dataset contains house sale prices for Kings County, which includes Seattle. It includes homes sold between May 2014 and May 2015. There are several versions of the data. Some additional information about the columns is available here: https://geodacenter.github.io/data-and-lab/KingCounty-HouseSales2015/, some of which are copied below."
      ],
      "metadata": {
        "id": "sHSOmwTF7LPK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **id**\tIdentification\n",
        "* **date**\tDate sold\n",
        "* **price**\tSale price\n",
        "* **bedrooms**\tNumber of bedrooms\n",
        "* **bathrooms**\tNumber of bathrooms\n",
        "* **sqft_liv**\tSize of living area in square feet\n",
        "* **sqft_lot**\tSize of the lot in square feet\n",
        "* **floors**\tNumber of floors\n",
        "* **waterfront**\tâ€˜1â€™ if the property has a waterfront, â€˜0â€™ if not\n",
        "* **view**\tAn index from 0 to 4 of how good the view of the property was\n",
        "* **condition**\tCondition of the house, ranked from 1 to 5\n",
        "* **grade**\tClassification by construction quality which refers to the types of materials used and the quality of workmanship. Buildings of better quality (higher grade) cost more to build per unit of measure and command higher value.\n",
        "* **sqft_above**\tSquare feet above ground\n",
        "* **sqft_basmt**\tSquare feet below ground\n",
        "* **yr_built**\tYear built\n",
        "* **yr_renov**\tYear renovated. â€˜0â€™ if never renovated\n",
        "* **zipcode**\t5 digit zip code\n",
        "* **lat**\tLatitude\n",
        "* **long**\tLongitude\n",
        "* **squft_liv15**\tAverage size of interior housing living space for the closest 15 houses, in square feet\n",
        "* **squft_lot15**\tAverage size of land lost for the closest 15 houses, in square feet"
      ],
      "metadata": {
        "id": "NIsgTeVwNf-1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the cells below we deal with all the Python imports that will be useful for our analysis."
      ],
      "metadata": {
        "id": "lFvYzVYElHGm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy as sp\n",
        "import scipy.stats as stats\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import copy\n",
        "sns.set()\n",
        "import statsmodels.formula.api as smf\n",
        "import statsmodels.api as sm\n",
        "# Any graph which we are creating as a part of our code must appear in the same notebook and not in separate window\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "8kA57oNYPd7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Munging Data**"
      ],
      "metadata": {
        "id": "pFC0m9aCQqg0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this part, let's load and inspect data. We will also learn how to transform columns when needed.\n",
        "\n",
        "\n",
        "> ðŸ’¡**Tip:** `pd.read_csv(<file path>)` reads a csv file and returns to pandas data frame object. It can also read files with other delimiter such as `.tsv` files. pandas also has `pd.read_excel` to read Excel files. See more in [this document](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NHfplVxcQwlK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('input_data/house_sales/kc_house_data.csv')"
      ],
      "metadata": {
        "id": "Z3ruSeXMQxwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Date string to numbers**"
      ],
      "metadata": {
        "id": "_MlrFGIJTWog"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's overview the dataframe. Using `.head()` on the dataframe, we can see the first 5 rows of the data. You can specify number of rows as argument then it will show those number of rows. similarly, `.tail()` gives the last 5 rows by default. You can see the columns names, but not all columns are displayed if there are too many columns.\n",
        "\n",
        "> ðŸ’¡**Tip:** If you want to show all columns and rows, there are [pandas command](https://www.geeksforgeeks.org/show-all-columns-of-pandas-dataframe-in-jupyter-notebook/) setting max rows and cols.\n",
        "\n",
        "The column 'date' is the date sold (with some timestamp as well), and the data is string type. Note that sometimes data tables may have date/time columns as datetime object types. In this example, it has a string type. We will extract year and month information from the string. In the data frame `df`, let's create new features 'sales_year' and 'sales_month' using 'date' column.\n",
        "\n",
        "> ðŸ’¡**Tip:** You can use either bracket (e.g. `df['date']`) or dot (e.g. `df.date`) to get the column `'date'` in the data frame `df`. A single columns object from dataframe is a pandas series object type, and you can use `.apply()` method for a transformation. `.apply()` is generic and can be applied to not only to single column (pandas series) but also to multiple columns (pandas dataframe). Here, we will apply it to a single column object and use `lambda` function inside the `.apply()` as shown below. You can find more examples, see the [doc](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.apply.html), which serves also as a precious reference.\n",
        "\n",
        "In this case, when we inspect the `'date'` column, it is a string object, so we can slice the year and month from the string. Also, we'd like to convert the extracted year and month strings to ingeters."
      ],
      "metadata": {
        "id": "8i76LwZYC1L_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "6NUqlguVVitT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With `iloc` (Pandas library), we can select and access data in DataFrames or Series using integer-based indexing. It allows you to specify rows and columns by their numerical indices.\n",
        "\n",
        "> ðŸ’¡**Tip:** Try to create a new code cell and select other columns within the `df` DataFrame, just to understand how `iloc` works. See [docs](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.iloc.html) for more reference."
      ],
      "metadata": {
        "id": "zU8wEat1Ftag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.date)\n",
        "print(type(df.date.iloc[0]))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "_95X7s0jWBu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extract year and month info from the string\n",
        "# create new features 'sales_year' and 'sales_month' in df\n",
        "\n",
        "df['sales_year'] = df.date.apply(lambda x: int(x[:4]))\n",
        "df['sales_month'] = df.date.apply(lambda x: int(x[4:6]))"
      ],
      "metadata": {
        "id": "CTv3CJsyWaPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupby('sales_month')"
      ],
      "metadata": {
        "id": "LxVHGGp_Wcsi",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's count how many sales occurred in each month and each year. We can use `.groupby()` function to group by 'sales_month' and 'sales_year' as shown below.\n",
        "\n",
        "> ðŸ’¡**Tip:** `.groupby()` itself returns an object that doesn't get displayed, hence not a processed dataframe. It is often used with some other aggregation method, such as `.count()`, `.mean()`, `.sum()`, etc.\n",
        "In the below example, we use `.count()` to count number of sales per each group (e.g. by sales_month). `.groupby()` can also group by multiple columns. This [resource](https://realpython.com/pandas-groupby/) has more explanations and examples, including some useful SQL analogies.\n",
        "\n",
        "In the following lines, by selecting a specific column after `groupby()`, you can apply `count()` to count non-null entries in that column only."
      ],
      "metadata": {
        "id": "4ADhWO9AWfVi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.groupby('sales_month')['id'].count())\n",
        "print(df.groupby('sales_year')['id'].count())"
      ],
      "metadata": {
        "id": "KS3m3RwtW3p_",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a new text cell and answer to the following questions.\n",
        "\n",
        "**Question 1.** Based on the output from above cell, which month has the most number of sales?\n",
        "\n",
        "**Question 2.** Which months has instead the least number of sales?"
      ],
      "metadata": {
        "id": "HkhJPA0FJKBD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's have a look at what data type each columns has. We can use .`info()` method on the dataframe object to see the data type. You can see `int64`, `float64` and `object` in our example. `object` can be string type or something else (such as list or other types of objects).\n",
        "\n",
        "> ðŸ’¡**Tip:** Note that sometimes raw data is not adequately formatted, so you might see columns that are supposed to be numbers typed as strings. It is **always a good practice** to firstly inspect columns data types and clean them if necessary."
      ],
      "metadata": {
        "id": "Wn0LNftLJ2Y5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Y0zm-yGjJRLd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Variable types**"
      ],
      "metadata": {
        "id": "Mp_CyiuBL5zX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Review Concepts**\n",
        "\n",
        "âœ”ï¸ What data types can be considered as a numeric variable?\n",
        "\n",
        "âœ”ï¸ What is the difference between ordinal and non-ordinal categorical variables?\n",
        "\n",
        "> ðŸ’¡**Tip:** Is binary categorical variable (Yes/No, Male/Female, True/False, Positive/Negative etc) numeric? Why or why not?\n",
        ">\n",
        "> How about a variable that has meaning of degree, such as survey/review ratings (very satisfied = 5, satisfied = 4, neutral = 3, disatisfied = 2, very disatisfied = 1)?\n",
        ">\n",
        "> Typically it is recommended to treat ordinal categorical variable (which order has meaning- e.g. degree, grades, numbers, severity etc) as numeric variable because a linear regression (or any ML) model can treat that variable (feature) as numbers and can learn a relationship to the t\n",
        "target variable y.\n",
        ">\n",
        ">Also, categorical variables sometimes need to be binarized (which involves to transform the column into multiple binary columns) before using them in a linear regression model (One-Hot Encoding).\n",
        "\n",
        "Inspect each feature's data type and variable type. What is the best description for the variable type of following features? Update the strings in the Code Cell below to 'numeric' or 'categorical'."
      ],
      "metadata": {
        "id": "lKRDKzNwM_Yd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# uncomment the feaures below and update the strings with 'numeric' or 'categorical'\n",
        "\n",
        "'''\n",
        "\n",
        "price = ''\n",
        "bathrooms = ''\n",
        "waterfront = ''\n",
        "grade = ''\n",
        "zipcode = ''\n",
        "sales_year = ''\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "collapsed": true,
        "id": "TPjh-TjkM-s5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ðŸ’¡**Tip:** Make use of `.unique()` method, referring to each column to get a glimpse of the unique values in each fature within the DataFrame.\n",
        "\n",
        "Try the cell below to check what unique values exist in each column (expect a pretty long output)."
      ],
      "metadata": {
        "id": "qBJBiibcQUkM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for c in df.columns[2:]:\n",
        "    print(c, df[c].unique())"
      ],
      "metadata": {
        "collapsed": true,
        "id": "1OzW7b1vQSy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Drop features**"
      ],
      "metadata": {
        "id": "zVBIoG5gRJ89"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's drop features that are unnecessary. `id` is not a meaningful feature. `date` string has been coded to `sales_month` and `sales_year`, so we can remove `date`. `zipcode` can be also removed as it's hard to include in a linear regression model and the location info is included in the `lat` and `long` (in case we need it). Drop the features `id`, `date`, and `zipcode` and replace the `df`.\n",
        "\n",
        "> ðŸ’¡**Tip:** `.drop()` function can drop one or more columns or rows. Some more suggestions how to use it in the [docs](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html)."
      ],
      "metadata": {
        "id": "1Q4jZ40kRk1E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# drop unnecessary features, replace df\n",
        "\n",
        "df = df.drop(columns=['id', 'date', 'zipcode'])"
      ],
      "metadata": {
        "id": "ozGZjmFhRR8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test that you droppd the features `id`, `date`, and `zipcode` from `df` by inspecting the dataframe."
      ],
      "metadata": {
        "id": "g-RvCu4lSQR0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **More inspection, Correlation and Pair-Plot**"
      ],
      "metadata": {
        "id": "166AS9K1TeVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Get correlation matrix on the data frame**"
      ],
      "metadata": {
        "id": "Qb2_881QVvwD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which feature may be the best predictor of price based on the correlation? Answer as a string value (e.g. best_guess_predictor = 'price' or best_guess_predictor = 'yr_built')\n",
        "\n",
        "> ðŸ’¡**Tip:** `.corr() `finction can show correlation matrix from the dataframe. More resource"
      ],
      "metadata": {
        "id": "XXAl54raV1RK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise**\n",
        "\n",
        "âœ”ï¸ By looking at the correlation matrix, how do you decide which feature is the best predictor?"
      ],
      "metadata": {
        "id": "R-Fj4rt-WN3R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.corr()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "N2HoDCCiTjiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# update best_guess_predictor with a string value (i.e. the feature name)\n",
        "best_guess_predictor = ''"
      ],
      "metadata": {
        "id": "LP8nkpgsWmTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Display the correlation matrix as heat map**"
      ],
      "metadata": {
        "id": "WNfemhClWwgv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The method `seaborn.heatmap()` can visualize a matrix as a heatmap. The Seaborn library allows you to easily create highly customized visualizations of your data, such as line plots, histograms, and heatmaps.\n",
        "\n",
        "In order to visualize the correlation matrix using `seaborn.heatmap()`, play with color map, text font size, decimals, text orientation etc. To have an overview of the different parameters to be set, the classic [docs](https://seaborn.pydata.org/generated/seaborn.heatmap.html) and this [cheatsheet](https://images.datacamp.com/image/upload/v1676302629/Marketing/Blog/Seaborn_Cheat_Sheet.pdf) come in handy.\n",
        "\n",
        "\n",
        "If you are satisfied with your method and you have produced a pretty visualization, please share it!"
      ],
      "metadata": {
        "id": "6vg6ihBHW7tk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corr_matrix=df.corr()\n",
        "plt.figure(figsize=(10, 9))\n",
        "sns.set(font_scale=1.2)\n",
        "sns.heatmap(corr_matrix, cmap=\"BuPu\")\n",
        "plt.title('CORRELATION HEATMAP')"
      ],
      "metadata": {
        "id": "62HhCehbW3je"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Pair plot**"
      ],
      "metadata": {
        "id": "hrc6EIMlbt7o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pair plot is a fast way to inspect relationships between features. Use seaborn's `.pairplot()` function to draw a pairplot if the first 10 columns (including price) and inspect their relationships. Set the diagonal elements to be KDE plot (setting the diagonal as \"KDE\" is a statistical trick useful to properly represent a probability density estimation). Be aware that the execution of this cell might take a little while."
      ],
      "metadata": {
        "id": "HtfVaPkgbywQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.pairplot(df, vars=df.columns[0:10], diag_kind='kde')"
      ],
      "metadata": {
        "id": "dCgwJJEGbxrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question**\n",
        "\n",
        "Do you see some correlation between variables? I so, between which variables?"
      ],
      "metadata": {
        "id": "fb_hwhvPcOS0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Simple Linear Regression**"
      ],
      "metadata": {
        "id": "Ur_shECudvw2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data preparation**"
      ],
      "metadata": {
        "id": "1zafQpPheJDN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will split the data to train and test datasets such that the test dataset is 20% of original data. Use `sklearn.model_selection.train_test_split` function to split the DataFrame to X_train and X_test.\n",
        "\n",
        "*   **X_train** is **80%** of observation randomly chosen.\n",
        "*   **X_test** is the rest **20%**.\n",
        "\n",
        "Both X_train and X_test are `pd.DataFrame` object and include `'price'` in the table. Note that the `train_test_split` can handle DataFrames as well as arrays.\n",
        "\n",
        "> ðŸ’¡**Tip:** Since we would like X_train to be 80% of the observation and X_test to be 20% of the observations, print length of X_train and X_test."
      ],
      "metadata": {
        "id": "465Yu1ybeNS1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test = train_test_split(df, test_size=0.2)\n",
        "print(\"lenght X_train =\", len(X_train))\n",
        "print(\"lenght X_test =\", len(X_test))"
      ],
      "metadata": {
        "id": "CPmUfR0hdx1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Train a simple linear regression model**"
      ],
      "metadata": {
        "id": "nZU2wBKlfWNh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the best_guess_predictor as a single predictor and build a simple linear regression model using **`statsmodels.formula.api.ols`** function (https://www.statsmodels.org/dev/example_formulas.html).\n",
        "\n",
        "Substitute the \"`best_predictor`\" part of the string as formula argument and run the model (trained on the X_train portion of our data). Thereafter, you can print out the result summary.\n",
        "\n",
        "> ðŸ’¡**Tip:** We had imported the library at the top of this notebook. So you can use the `smf` alias.\n",
        ">\n",
        ">**`import statsmodels.formula.api as smf`**\n",
        "\n",
        "**N.B.:** It is recommended that you use the `statsmodel` library to do the regression analysis as opposed to others like `sklearn`. The `sklearn` library is great for advanced topics, but it's easier to get lost in a sea of details and it's not needed for these kind of problems.\n",
        "\n"
      ],
      "metadata": {
        "id": "ykTALlBxfe9T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = smf.ols(formula='price~grade', data=X_train).fit()\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "khpCQ0CRfZVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question**\n",
        "\n",
        "What is the adjusted R-squared value?"
      ],
      "metadata": {
        "id": "h4JpGRVwhseY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Best predictor**"
      ],
      "metadata": {
        "id": "FpjazabThzpv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the previous section, we picked a best guess predictor for price based on the correlation matrix. Now we will consider whether the best_guess_predictor that we used is still the best.\n",
        "\n",
        "You now have to iteratively try the model with the other features having higher correlation value and see the summary outcome (replace string \"`try_predictor`\" in the cell below).\n",
        "\n",
        "As a final task, print out a list of the top three predictors in decreasing order.\n",
        "\n",
        ">ðŸ’¡**Tip:** Linear regression uses adjusted R squared as fit performance, so you can rank by this metric. Complete the `predictors_r2` dictionary taking note of adjusted R2 and then derive a ranking.\n",
        "\n"
      ],
      "metadata": {
        "id": "6_dQIUdBh2Jw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# try the model with the other features with higher correlation value\n",
        "\n",
        "model = smf.ols(formula='price~<try_predictor>', data=X_train).fit()\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "qZrwQHg8iUwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# complete dictionary\n",
        "\n",
        "'''\n",
        "predictors_r2 = {\n",
        "    'feature_1': 0.000,\n",
        "    'feature_2': 0.000,\n",
        "}\n",
        "'''\n",
        "\n",
        "# update top_three\n",
        "\n",
        "top_three = ['feature_1','feature_2','feature_3']"
      ],
      "metadata": {
        "id": "RnZBrPW7nSSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Questions**\n",
        "\n",
        "*  What were your top three predictors?\n",
        "*  How did you order your list of predictors to select those as the top ones?\n",
        "*  Is your top predictor for this section the same as the best guess predictor you selected before?"
      ],
      "metadata": {
        "id": "CRMNpvlZiJFQ"
      }
    }
  ]
}